{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, RegressorMixin\n",
    "\n",
    "\n",
    "\n",
    "def batch_generator(batch_size, data, labels=None):\n",
    "    \"\"\"\n",
    "    Generates batches of samples\n",
    "    :param data: array-like, shape = (n_samples, n_features)\n",
    "    :param labels: array-like, shape = (n_samples, )\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    n_batches = int(np.ceil(len(data) / float(batch_size)))\n",
    "    idx = np.random.permutation(len(data))\n",
    "    data_shuffled = data[idx]\n",
    "    if labels is not None:\n",
    "        labels_shuffled = labels[idx]\n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        if labels is not None:\n",
    "            yield data_shuffled[start:end, :], labels_shuffled[start:end]\n",
    "        else:\n",
    "            yield data_shuffled[start:end, :]\n",
    "\n",
    "\n",
    "def to_categorical(labels, num_classes):\n",
    "    \"\"\"\n",
    "    Converts labels as single integer to row vectors. For instance, given a three class problem, labels would be\n",
    "    mapped as label_1: [1 0 0], label_2: [0 1 0], label_3: [0, 0, 1] where labels can be either int or string.\n",
    "    :param labels: array-like, shape = (n_samples, )\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    new_labels = np.zeros([len(labels), num_classes])\n",
    "    label_to_idx_map, idx_to_label_map = dict(), dict()\n",
    "    idx = 0\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in label_to_idx_map:\n",
    "            label_to_idx_map[label] = idx\n",
    "            idx_to_label_map[idx] = label\n",
    "            idx += 1\n",
    "        new_labels[i][label_to_idx_map[label]] = 1\n",
    "    return new_labels, label_to_idx_map, idx_to_label_map\n",
    "\n",
    "\n",
    "class ActivationFunction(object):\n",
    "    \"\"\"\n",
    "    Class for abstract activation function.\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    @abstractmethod\n",
    "    def function(self, x):\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def prime(self, x):\n",
    "        return\n",
    "\n",
    "\n",
    "class SigmoidActivationFunction(ActivationFunction):\n",
    "    @classmethod\n",
    "    def function(cls, x):\n",
    "        \"\"\"\n",
    "        Sigmoid function.\n",
    "        :param x: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return 1 / (1.0 + np.exp(-x))\n",
    "\n",
    "    @classmethod\n",
    "    def prime(cls, x):\n",
    "        \"\"\"\n",
    "        Compute sigmoid first derivative.\n",
    "        :param x: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "\n",
    "class ReLUActivationFunction(ActivationFunction):\n",
    "    @classmethod\n",
    "    def function(cls, x):\n",
    "        \"\"\"\n",
    "        Rectified linear function.\n",
    "        :param x: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.maximum(np.zeros(x.shape), x)\n",
    "\n",
    "    @classmethod\n",
    "    def prime(cls, x):\n",
    "        \"\"\"\n",
    "        Rectified linear first derivative.\n",
    "        :param x: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return (x > 0).astype(int)\n",
    "\n",
    "\n",
    "class TanhActivationFunction(ActivationFunction):\n",
    "    @classmethod\n",
    "    def function(cls, x):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent function.\n",
    "        :param x: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @classmethod\n",
    "    def prime(cls, x):\n",
    "        \"\"\"\n",
    "        Hyperbolic tangent first derivative.\n",
    "        :param x: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return 1 - x * x\n",
    "\n",
    "\n",
    "class BaseModel(object):\n",
    "    def save(self, save_path):\n",
    "        import pickle\n",
    "\n",
    "        with open(save_path, 'wb') as fp:\n",
    "            pickle.dump(self, fp)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, load_path):\n",
    "        import pickle\n",
    "\n",
    "        with open(load_path, 'rb') as fp:\n",
    "            return pickle.load(fp)\n",
    "\n",
    "\n",
    "class BinaryRBM(BaseEstimator, TransformerMixin, BaseModel):\n",
    "    \"\"\"\n",
    "    This class implements a Binary Restricted Boltzmann machine.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_hidden_units=100,\n",
    "                 activation_function='sigmoid',\n",
    "                 optimization_algorithm='sgd',\n",
    "                 learning_rate=1e-3,\n",
    "                 n_epochs=10,\n",
    "                 contrastive_divergence_iter=1,\n",
    "                 batch_size=32,\n",
    "                 verbose=True):\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.activation_function = activation_function\n",
    "        self.optimization_algorithm = optimization_algorithm\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.contrastive_divergence_iter = contrastive_divergence_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit a model given data.\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Initialize RBM parameters\n",
    "        self.n_visible_units = X.shape[1]\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            self.W = np.random.randn(self.n_hidden_units, self.n_visible_units) / np.sqrt(self.n_visible_units)\n",
    "            self.c = np.random.randn(self.n_hidden_units) / np.sqrt(self.n_visible_units)\n",
    "            self.b = np.random.randn(self.n_visible_units) / np.sqrt(self.n_visible_units)\n",
    "            self._activation_function_class = SigmoidActivationFunction\n",
    "        elif self.activation_function == 'relu':\n",
    "            self.W = truncnorm.rvs(-0.2, 0.2, size=[self.n_hidden_units, self.n_visible_units]) / np.sqrt(\n",
    "                self.n_visible_units)\n",
    "            self.c = np.full(self.n_hidden_units, 0.1) / np.sqrt(self.n_visible_units)\n",
    "            self.b = np.full(self.n_visible_units, 0.1) / np.sqrt(self.n_visible_units)\n",
    "            self._activation_function_class = ReLUActivationFunction\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function.\")\n",
    "\n",
    "        if self.optimization_algorithm == 'sgd':\n",
    "            self._stochastic_gradient_descent(X)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimization algorithm.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms data using the fitted model.\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 1:  # It is a single sample\n",
    "            return self._compute_hidden_units(X)\n",
    "        transformed_data = self._compute_hidden_units_matrix(X)\n",
    "        return transformed_data\n",
    "\n",
    "    def _reconstruct(self, transformed_data):\n",
    "        \"\"\"\n",
    "        Reconstruct visible units given the hidden layer output.\n",
    "        :param transformed_data: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self._compute_visible_units_matrix(transformed_data)\n",
    "\n",
    "    def _stochastic_gradient_descent(self, _data):\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient descend optimization algorithm.\n",
    "        :param _data: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        accum_delta_W = np.zeros(self.W.shape)\n",
    "        accum_delta_b = np.zeros(self.b.shape)\n",
    "        accum_delta_c = np.zeros(self.c.shape)\n",
    "        for iteration in range(1, self.n_epochs + 1):\n",
    "            idx = np.random.permutation(len(_data))\n",
    "            data = _data[idx]\n",
    "            for batch in batch_generator(self.batch_size, data):\n",
    "                accum_delta_W[:] = .0\n",
    "                accum_delta_b[:] = .0\n",
    "                accum_delta_c[:] = .0\n",
    "                for sample in batch:\n",
    "                    delta_W, delta_b, delta_c = self._contrastive_divergence(sample)\n",
    "                    accum_delta_W += delta_W\n",
    "                    accum_delta_b += delta_b\n",
    "                    accum_delta_c += delta_c\n",
    "                self.W += self.learning_rate * (accum_delta_W / self.batch_size)\n",
    "                self.b += self.learning_rate * (accum_delta_b / self.batch_size)\n",
    "                self.c += self.learning_rate * (accum_delta_c / self.batch_size)\n",
    "            if self.verbose:\n",
    "                error = self._compute_reconstruction_error(data)\n",
    "                print(\">> Epoch %d finished \\tRBM Reconstruction error %f\" % (iteration, error))\n",
    "\n",
    "    def _contrastive_divergence(self, vector_visible_units):\n",
    "        \"\"\"\n",
    "        Computes gradients using Contrastive Divergence method.\n",
    "        :param vector_visible_units: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        v_0 = vector_visible_units\n",
    "        v_t = np.array(v_0)\n",
    "\n",
    "        # Sampling\n",
    "        for t in range(self.contrastive_divergence_iter):\n",
    "            h_t = self._sample_hidden_units(v_t)\n",
    "            v_t = self._compute_visible_units(h_t)\n",
    "\n",
    "        # Computing deltas\n",
    "        v_k = v_t\n",
    "        h_0 = self._compute_hidden_units(v_0)\n",
    "        h_k = self._compute_hidden_units(v_k)\n",
    "        delta_W = np.outer(h_0, v_0) - np.outer(h_k, v_k)\n",
    "        delta_b = v_0 - v_k\n",
    "        delta_c = h_0 - h_k\n",
    "\n",
    "        return delta_W, delta_b, delta_c\n",
    "\n",
    "    def _sample_hidden_units(self, vector_visible_units):\n",
    "        \"\"\"\n",
    "        Computes hidden unit activations by sampling from a binomial distribution.\n",
    "        :param vector_visible_units: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden_units = self._compute_hidden_units(vector_visible_units)\n",
    "        return (np.random.random_sample(len(hidden_units)) < hidden_units).astype(np.int64)\n",
    "\n",
    "    def _sample_visible_units(self, vector_hidden_units):\n",
    "        \"\"\"\n",
    "        Computes visible unit activations by sampling from a binomial distribution.\n",
    "        :param vector_hidden_units: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        visible_units = self._compute_visible_units(vector_hidden_units)\n",
    "        return (np.random.random_sample(len(visible_units)) < visible_units).astype(np.int64)\n",
    "\n",
    "    def _compute_hidden_units(self, vector_visible_units):\n",
    "        \"\"\"\n",
    "        Computes hidden unit outputs.\n",
    "        :param vector_visible_units: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        v = np.expand_dims(vector_visible_units, 0)\n",
    "        h = np.squeeze(self._compute_hidden_units_matrix(v))\n",
    "        return np.array([h]) if not h.shape else h\n",
    "\n",
    "    def _compute_hidden_units_matrix(self, matrix_visible_units):\n",
    "        \"\"\"\n",
    "        Computes hidden unit outputs.\n",
    "        :param matrix_visible_units: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.transpose(self._activation_function_class.function(\n",
    "            np.dot(self.W, np.transpose(matrix_visible_units)) + self.c[:, np.newaxis]))\n",
    "\n",
    "    def _compute_visible_units(self, vector_hidden_units):\n",
    "        \"\"\"\n",
    "        Computes visible (or input) unit outputs.\n",
    "        :param vector_hidden_units: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        h = np.expand_dims(vector_hidden_units, 0)\n",
    "        v = np.squeeze(self._compute_visible_units_matrix(h))\n",
    "        return np.array([v]) if not v.shape else v\n",
    "\n",
    "    def _compute_visible_units_matrix(self, matrix_hidden_units):\n",
    "        \"\"\"\n",
    "        Computes visible (or input) unit outputs.\n",
    "        :param matrix_hidden_units: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self._activation_function_class.function(np.dot(matrix_hidden_units, self.W) + self.b[np.newaxis, :])\n",
    "\n",
    "    def _compute_free_energy(self, vector_visible_units):\n",
    "        \"\"\"\n",
    "        Computes the RBM free energy.\n",
    "        :param vector_visible_units: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        v = vector_visible_units\n",
    "        return - np.dot(self.b, v) - np.sum(np.log(1 + np.exp(np.dot(self.W, v) + self.c)))\n",
    "\n",
    "    def _compute_reconstruction_error(self, data):\n",
    "        \"\"\"\n",
    "        Computes the reconstruction error of the data.\n",
    "        :param data: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        data_transformed = self.transform(data)\n",
    "        data_reconstructed = self._reconstruct(data_transformed)\n",
    "        return np.mean(np.sum((data_reconstructed - data) ** 2, 1))\n",
    "\n",
    "\n",
    "class UnsupervisedDBN(BaseEstimator, TransformerMixin, BaseModel):\n",
    "    \"\"\"\n",
    "    This class implements a unsupervised Deep Belief Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_layers_structure=[100, 100],\n",
    "                 activation_function='sigmoid',\n",
    "                 optimization_algorithm='sgd',\n",
    "                 learning_rate_rbm=1e-3,\n",
    "                 n_epochs_rbm=10,\n",
    "                 contrastive_divergence_iter=1,\n",
    "                 batch_size=32,\n",
    "                 verbose=True):\n",
    "        self.hidden_layers_structure = hidden_layers_structure\n",
    "        self.activation_function = activation_function\n",
    "        self.optimization_algorithm = optimization_algorithm\n",
    "        self.learning_rate_rbm = learning_rate_rbm\n",
    "        self.n_epochs_rbm = n_epochs_rbm\n",
    "        self.contrastive_divergence_iter = contrastive_divergence_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.rbm_layers = None\n",
    "        self.verbose = verbose\n",
    "        self.rbm_class = BinaryRBM\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits a model given data.\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Initialize rbm layers\n",
    "        self.rbm_layers = list()\n",
    "        for n_hidden_units in self.hidden_layers_structure:\n",
    "            rbm = self.rbm_class(n_hidden_units=n_hidden_units,\n",
    "                                 activation_function=self.activation_function,\n",
    "                                 optimization_algorithm=self.optimization_algorithm,\n",
    "                                 learning_rate=self.learning_rate_rbm,\n",
    "                                 n_epochs=self.n_epochs_rbm,\n",
    "                                 contrastive_divergence_iter=self.contrastive_divergence_iter,\n",
    "                                 batch_size=self.batch_size,\n",
    "                                 verbose=self.verbose)\n",
    "            self.rbm_layers.append(rbm)\n",
    "\n",
    "        # Fit RBM\n",
    "        if self.verbose:\n",
    "            print(\"[START] Pre-training step:\")\n",
    "        input_data = X\n",
    "        for rbm in self.rbm_layers:\n",
    "            rbm.fit(input_data)\n",
    "            input_data = rbm.transform(input_data)\n",
    "        if self.verbose:\n",
    "            print(\"[END] Pre-training step\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms data using the fitted model.\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        input_data = X\n",
    "        for rbm in self.rbm_layers:\n",
    "            input_data = rbm.transform(input_data)\n",
    "        return input_data\n",
    "\n",
    "\n",
    "class AbstractSupervisedDBN(BaseEstimator, BaseModel):\n",
    "    \"\"\"\n",
    "    Abstract class for supervised Deep Belief Network.\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self,\n",
    "                 unsupervised_dbn_class,\n",
    "                 hidden_layers_structure=[100, 100],\n",
    "                 activation_function='sigmoid',\n",
    "                 optimization_algorithm='sgd',\n",
    "                 learning_rate=1e-3,\n",
    "                 learning_rate_rbm=1e-3,\n",
    "                 n_iter_backprop=100,\n",
    "                 l2_regularization=1.0,\n",
    "                 n_epochs_rbm=10,\n",
    "                 contrastive_divergence_iter=1,\n",
    "                 batch_size=32,\n",
    "                 dropout_p=0,  # float between 0 and 1. Fraction of the input units to drop\n",
    "                 verbose=True):\n",
    "        self.unsupervised_dbn = unsupervised_dbn_class(hidden_layers_structure=hidden_layers_structure,\n",
    "                                                       activation_function=activation_function,\n",
    "                                                       optimization_algorithm=optimization_algorithm,\n",
    "                                                       learning_rate_rbm=learning_rate_rbm,\n",
    "                                                       n_epochs_rbm=n_epochs_rbm,\n",
    "                                                       contrastive_divergence_iter=contrastive_divergence_iter,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       verbose=verbose)\n",
    "        self.unsupervised_dbn_class = unsupervised_dbn_class\n",
    "        self.n_iter_backprop = n_iter_backprop\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.p = 1 - self.dropout_p\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y=None, pre_train=True):\n",
    "        \"\"\"\n",
    "        Fits a model given data.\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :param y : array-like, shape = (n_samples, )\n",
    "        :param pre_train: bool\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if pre_train:\n",
    "            self.pre_train(X)\n",
    "        self._fine_tuning(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the target given data.\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 1:  # It is a single sample\n",
    "            X = np.expand_dims(X, 0)\n",
    "        transformed_data = self.transform(X)\n",
    "        predicted_data = self._compute_output_units_matrix(transformed_data)\n",
    "        return predicted_data\n",
    "\n",
    "    def pre_train(self, X):\n",
    "        \"\"\"\n",
    "        Apply unsupervised network pre-training.\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.unsupervised_dbn.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, *args):\n",
    "        return self.unsupervised_dbn.transform(*args)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _transform_labels_to_network_format(self, labels):\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def _compute_output_units_matrix(self, matrix_visible_units):\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def _determine_num_output_neurons(self, labels):\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def _stochastic_gradient_descent(self, data, labels):\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def _fine_tuning(self, data, _labels):\n",
    "        return\n",
    "\n",
    "\n",
    "class NumPyAbstractSupervisedDBN(AbstractSupervisedDBN):\n",
    "    \"\"\"\n",
    "    Abstract class for supervised Deep Belief Network in NumPy\n",
    "    \"\"\"\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NumPyAbstractSupervisedDBN, self).__init__(UnsupervisedDBN, **kwargs)\n",
    "\n",
    "    def _compute_activations(self, sample):\n",
    "        \"\"\"\n",
    "        Compute output values of all layers.\n",
    "        :param sample: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        input_data = sample\n",
    "        if self.dropout_p > 0:\n",
    "            r = np.random.binomial(1, self.p, len(input_data))\n",
    "            input_data *= r\n",
    "        layers_activation = list()\n",
    "\n",
    "        for rbm in self.unsupervised_dbn.rbm_layers:\n",
    "            input_data = rbm.transform(input_data)\n",
    "            if self.dropout_p > 0:\n",
    "                r = np.random.binomial(1, self.p, len(input_data))\n",
    "                input_data *= r\n",
    "            layers_activation.append(input_data)\n",
    "\n",
    "        # Computing activation of output layer\n",
    "        input_data = self._compute_output_units(input_data)\n",
    "        layers_activation.append(input_data)\n",
    "\n",
    "        return layers_activation\n",
    "\n",
    "    def _stochastic_gradient_descent(self, _data, _labels):\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient descend optimization algorithm.\n",
    "        :param _data: array-like, shape = (n_samples, n_features)\n",
    "        :param _labels: array-like, shape = (n_samples, targets)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            matrix_error = np.zeros([len(_data), self.num_classes])\n",
    "        num_samples = len(_data)\n",
    "        accum_delta_W = [np.zeros(rbm.W.shape) for rbm in self.unsupervised_dbn.rbm_layers]\n",
    "        accum_delta_W.append(np.zeros(self.W.shape))\n",
    "        accum_delta_bias = [np.zeros(rbm.c.shape) for rbm in self.unsupervised_dbn.rbm_layers]\n",
    "        accum_delta_bias.append(np.zeros(self.b.shape))\n",
    "\n",
    "        for iteration in range(1, self.n_iter_backprop + 1):\n",
    "            idx = np.random.permutation(len(_data))\n",
    "            data = _data[idx]\n",
    "            labels = _labels[idx]\n",
    "            i = 0\n",
    "            for batch_data, batch_labels in batch_generator(self.batch_size, data, labels):\n",
    "                # Clear arrays\n",
    "                for arr1, arr2 in zip(accum_delta_W, accum_delta_bias):\n",
    "                    arr1[:], arr2[:] = .0, .0\n",
    "                for sample, label in zip(batch_data, batch_labels):\n",
    "                    delta_W, delta_bias, predicted = self._backpropagation(sample, label)\n",
    "                    for layer in range(len(self.unsupervised_dbn.rbm_layers) + 1):\n",
    "                        accum_delta_W[layer] += delta_W[layer]\n",
    "                        accum_delta_bias[layer] += delta_bias[layer]\n",
    "                    if self.verbose:\n",
    "                        loss = self._compute_loss(predicted, label)\n",
    "                        matrix_error[i, :] = loss\n",
    "                        i += 1\n",
    "\n",
    "                layer = 0\n",
    "                for rbm in self.unsupervised_dbn.rbm_layers:\n",
    "                    # Updating parameters of hidden layers\n",
    "                    rbm.W = (1 - (\n",
    "                        self.learning_rate * self.l2_regularization) / num_samples) * rbm.W - self.learning_rate * (\n",
    "                        accum_delta_W[layer] / self.batch_size)\n",
    "                    rbm.c -= self.learning_rate * (accum_delta_bias[layer] / self.batch_size)\n",
    "                    layer += 1\n",
    "                # Updating parameters of output layer\n",
    "                self.W = (1 - (\n",
    "                    self.learning_rate * self.l2_regularization) / num_samples) * self.W - self.learning_rate * (\n",
    "                    accum_delta_W[layer] / self.batch_size)\n",
    "                self.b -= self.learning_rate * (accum_delta_bias[layer] / self.batch_size)\n",
    "\n",
    "            if self.verbose:\n",
    "                error = np.mean(np.sum(matrix_error, 1))\n",
    "                print(\">> Epoch %d finished \\tANN training loss %f\" % (iteration, error))\n",
    "\n",
    "    def _backpropagation(self, input_vector, label):\n",
    "        \"\"\"\n",
    "        Performs Backpropagation algorithm for computing gradients.\n",
    "        :param input_vector: array-like, shape = (n_features, )\n",
    "        :param label: array-like, shape = (n_targets, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x, y = input_vector, label\n",
    "        deltas = list()\n",
    "        list_layer_weights = list()\n",
    "        for rbm in self.unsupervised_dbn.rbm_layers:\n",
    "            list_layer_weights.append(rbm.W)\n",
    "        list_layer_weights.append(self.W)\n",
    "\n",
    "        # Forward pass\n",
    "        layers_activation = self._compute_activations(input_vector)\n",
    "\n",
    "        # Backward pass: computing deltas\n",
    "        activation_output_layer = layers_activation[-1]\n",
    "        delta_output_layer = self._compute_output_layer_delta(y, activation_output_layer)\n",
    "        deltas.append(delta_output_layer)\n",
    "        layer_idx = list(range(len(self.unsupervised_dbn.rbm_layers)))\n",
    "        layer_idx.reverse()\n",
    "        delta_previous_layer = delta_output_layer\n",
    "        for layer in layer_idx:\n",
    "            neuron_activations = layers_activation[layer]\n",
    "            W = list_layer_weights[layer + 1]\n",
    "            delta = np.dot(delta_previous_layer, W) * self.unsupervised_dbn.rbm_layers[\n",
    "                layer]._activation_function_class.prime(neuron_activations)\n",
    "            deltas.append(delta)\n",
    "            delta_previous_layer = delta\n",
    "        deltas.reverse()\n",
    "\n",
    "        # Computing gradients\n",
    "        layers_activation.pop()\n",
    "        layers_activation.insert(0, input_vector)\n",
    "        layer_gradient_weights, layer_gradient_bias = list(), list()\n",
    "        for layer in range(len(list_layer_weights)):\n",
    "            neuron_activations = layers_activation[layer]\n",
    "            delta = deltas[layer]\n",
    "            gradient_W = np.outer(delta, neuron_activations)\n",
    "            layer_gradient_weights.append(gradient_W)\n",
    "            layer_gradient_bias.append(delta)\n",
    "\n",
    "        return layer_gradient_weights, layer_gradient_bias, activation_output_layer\n",
    "\n",
    "    def _fine_tuning(self, data, _labels):\n",
    "        \"\"\"\n",
    "        Entry point of the fine tuning procedure.\n",
    "        :param data: array-like, shape = (n_samples, n_features)\n",
    "        :param _labels: array-like, shape = (n_samples, targets)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.num_classes = self._determine_num_output_neurons(_labels)\n",
    "        n_hidden_units_previous_layer = self.unsupervised_dbn.rbm_layers[-1].n_hidden_units\n",
    "        self.W = np.random.randn(self.num_classes, n_hidden_units_previous_layer) / np.sqrt(\n",
    "            n_hidden_units_previous_layer)\n",
    "        self.b = np.random.randn(self.num_classes) / np.sqrt(n_hidden_units_previous_layer)\n",
    "\n",
    "        labels = self._transform_labels_to_network_format(_labels)\n",
    "\n",
    "        # Scaling up weights obtained from pretraining\n",
    "        for rbm in self.unsupervised_dbn.rbm_layers:\n",
    "            rbm.W /= self.p\n",
    "            rbm.c /= self.p\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"[START] Fine tuning step:\")\n",
    "\n",
    "        if self.unsupervised_dbn.optimization_algorithm == 'sgd':\n",
    "            self._stochastic_gradient_descent(data, labels)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimization algorithm.\")\n",
    "\n",
    "        # Scaling down weights obtained from pretraining\n",
    "        for rbm in self.unsupervised_dbn.rbm_layers:\n",
    "            rbm.W *= self.p\n",
    "            rbm.c *= self.p\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"[END] Fine tuning step\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def _compute_loss(self, predicted, label):\n",
    "        return\n",
    "\n",
    "    @abstractmethod\n",
    "    def _compute_output_layer_delta(self, label, predicted):\n",
    "        return\n",
    "\n",
    "\n",
    "class SupervisedDBNClassification(NumPyAbstractSupervisedDBN, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    This class implements a Deep Belief Network for classification problems.\n",
    "    It appends a Softmax Linear Classifier as output layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def _transform_labels_to_network_format(self, labels):\n",
    "        \"\"\"\n",
    "        Converts labels as single integer to row vectors. For instance, given a three class problem, labels would be\n",
    "        mapped as label_1: [1 0 0], label_2: [0 1 0], label_3: [0, 0, 1] where labels can be either int or string.\n",
    "        :param labels: array-like, shape = (n_samples, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_labels = np.zeros([len(labels), self.num_classes])\n",
    "        self.label_to_idx_map, self.idx_to_label_map = dict(), dict()\n",
    "        idx = 0\n",
    "        for i, label in enumerate(labels):\n",
    "            if label not in self.label_to_idx_map:\n",
    "                self.label_to_idx_map[label] = idx\n",
    "                self.idx_to_label_map[idx] = label\n",
    "                idx += 1\n",
    "            new_labels[i][self.label_to_idx_map[label]] = 1\n",
    "        return new_labels\n",
    "\n",
    "    def _transform_network_format_to_labels(self, indexes):\n",
    "        \"\"\"\n",
    "        Converts network output to original labels.\n",
    "        :param indexes: array-like, shape = (n_samples, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return list(map(lambda idx: self.idx_to_label_map[idx], indexes))\n",
    "\n",
    "    def _compute_output_units(self, vector_visible_units):\n",
    "        \"\"\"\n",
    "        Compute activations of output units.\n",
    "        :param vector_visible_units: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        v = vector_visible_units\n",
    "        scores = np.dot(self.W, v) + self.b\n",
    "        # get unnormalized probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        # normalize them for each example\n",
    "        return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    def _compute_output_units_matrix(self, matrix_visible_units):\n",
    "        \"\"\"\n",
    "        Compute activations of output units.\n",
    "        :param matrix_visible_units: shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        matrix_scores = np.transpose(np.dot(self.W, np.transpose(matrix_visible_units)) + self.b[:, np.newaxis])\n",
    "        exp_scores = np.exp(matrix_scores)\n",
    "        return exp_scores / np.expand_dims(np.sum(exp_scores, axis=1), 1)\n",
    "\n",
    "    def _compute_output_layer_delta(self, label, predicted):\n",
    "        \"\"\"\n",
    "        Compute deltas of the output layer, using cross-entropy cost function.\n",
    "        :param label: array-like, shape = (n_features, )\n",
    "        :param predicted: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        dscores = np.array(predicted)\n",
    "        dscores[np.where(label == 1)] -= 1\n",
    "        return dscores\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicts probability distribution of classes for each sample in the given data.\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return super(SupervisedDBNClassification, self).predict(X)\n",
    "\n",
    "    def predict_proba_dict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts probability distribution of classes for each sample in the given data.\n",
    "        Returns a list of dictionaries, one per sample. Each dict contains {label_1: prob_1, ..., label_j: prob_j}\n",
    "        :param X: array-like, shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(X.shape) == 1:  # It is a single sample\n",
    "            X = np.expand_dims(X, 0)\n",
    "\n",
    "        predicted_probs = self.predict_proba(X)\n",
    "\n",
    "        result = []\n",
    "        num_of_data, num_of_labels = predicted_probs.shape\n",
    "        for i in range(num_of_data):\n",
    "            # key : label\n",
    "            # value : predicted probability\n",
    "            dict_prob = {}\n",
    "            for j in range(num_of_labels):\n",
    "                dict_prob[self.idx_to_label_map[j]] = predicted_probs[i][j]\n",
    "            result.append(dict_prob)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_proba(X)\n",
    "        indexes = np.argmax(probs, axis=1)\n",
    "        return self._transform_network_format_to_labels(indexes)\n",
    "\n",
    "    def _determine_num_output_neurons(self, labels):\n",
    "        \"\"\"\n",
    "        Given labels, compute the needed number of output units.\n",
    "        :param labels: shape = (n_samples, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return len(np.unique(labels))\n",
    "\n",
    "    def _compute_loss(self, probs, label):\n",
    "        \"\"\"\n",
    "        Computes categorical cross-entropy loss\n",
    "        :param probs:\n",
    "        :param label:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return -np.log(probs[np.where(label == 1)])\n",
    "\n",
    "\n",
    "class SupervisedDBNRegression(NumPyAbstractSupervisedDBN, RegressorMixin):\n",
    "    \"\"\"\n",
    "    This class implements a Deep Belief Network for regression problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def _transform_labels_to_network_format(self, labels):\n",
    "        \"\"\"\n",
    "        Returns the same labels since regression case does not need to convert anything.\n",
    "        :param labels: array-like, shape = (n_samples, targets)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return labels\n",
    "\n",
    "    def _compute_output_units(self, vector_visible_units):\n",
    "        \"\"\"\n",
    "        Compute activations of output units.\n",
    "        :param vector_visible_units: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        v = vector_visible_units\n",
    "        return np.dot(self.W, v) + self.b\n",
    "\n",
    "    def _compute_output_units_matrix(self, matrix_visible_units):\n",
    "        \"\"\"\n",
    "        Compute activations of output units.\n",
    "        :param matrix_visible_units: shape = (n_samples, n_features)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.transpose(np.dot(self.W, np.transpose(matrix_visible_units)) + self.b[:, np.newaxis])\n",
    "\n",
    "    def _compute_output_layer_delta(self, label, predicted):\n",
    "        \"\"\"\n",
    "        Compute deltas of the output layer for the regression case, using common (one-half) squared-error cost function.\n",
    "        :param label: array-like, shape = (n_features, )\n",
    "        :param predicted: array-like, shape = (n_features, )\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return -(label - predicted)\n",
    "\n",
    "    def _determine_num_output_neurons(self, labels):\n",
    "        \"\"\"\n",
    "        Given labels, compute the needed number of output units.\n",
    "        :param labels: shape = (n_samples, n_targets)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if len(labels.shape) == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return labels.shape[1]\n",
    "\n",
    "    def _compute_loss(self, predicted, label):\n",
    "        \"\"\"\n",
    "        Computes Mean squared error loss.\n",
    "        :param predicted:\n",
    "        :param label:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        error = predicted - label\n",
    "        return error * error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 43)\n",
      "(125973, 42)\n",
      "(125973, 123)\n",
      "X_train has shape: (125973, 122) \n",
      "y_train has shape: (125973,)\n",
      "[START] Pre-training step:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\file c\\Tai lieu\\IOT VA UNG DUNG\\TEST\\env\\lib\\site-packages\\numpy\\core\\numeric.py:942: RuntimeWarning: invalid value encountered in multiply\n",
      "  return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n",
      "e:\\file c\\Tai lieu\\IOT VA UNG DUNG\\TEST\\env\\lib\\site-packages\\numpy\\core\\numeric.py:942: RuntimeWarning: overflow encountered in multiply\n",
      "  return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 101\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39m# Build the DBN model\u001b[39;00m\n\u001b[0;32m     93\u001b[0m model \u001b[39m=\u001b[39m SupervisedDBNClassification(hidden_layers_structure\u001b[39m=\u001b[39m[\u001b[39m256\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m64\u001b[39m],\n\u001b[0;32m     94\u001b[0m             learning_rate_rbm\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m,\n\u001b[0;32m     95\u001b[0m             learning_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m             activation_function\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    100\u001b[0m             dropout_p\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m--> 101\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m    103\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m    104\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn [26], line 449\u001b[0m, in \u001b[0;36mAbstractSupervisedDBN.fit\u001b[1;34m(self, X, y, pre_train)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39mFits a model given data.\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39m:param X: array-like, shape = (n_samples, n_features)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m pre_train:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpre_train(X)\n\u001b[0;32m    450\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fine_tuning(X, y)\n\u001b[0;32m    451\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "Cell \u001b[1;32mIn [26], line 471\u001b[0m, in \u001b[0;36mAbstractSupervisedDBN.pre_train\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpre_train\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    466\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    Apply unsupervised network pre-training.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m    :param X: array-like, shape = (n_samples, n_features)\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m    :return:\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 471\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munsupervised_dbn\u001b[39m.\u001b[39;49mfit(X)\n\u001b[0;32m    472\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "Cell \u001b[1;32mIn [26], line 385\u001b[0m, in \u001b[0;36mUnsupervisedDBN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    383\u001b[0m input_data \u001b[39m=\u001b[39m X\n\u001b[0;32m    384\u001b[0m \u001b[39mfor\u001b[39;00m rbm \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrbm_layers:\n\u001b[1;32m--> 385\u001b[0m     rbm\u001b[39m.\u001b[39;49mfit(input_data)\n\u001b[0;32m    386\u001b[0m     input_data \u001b[39m=\u001b[39m rbm\u001b[39m.\u001b[39mtransform(input_data)\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n",
      "Cell \u001b[1;32mIn [26], line 185\u001b[0m, in \u001b[0;36mBinaryRBM.fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid activation function.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimization_algorithm \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 185\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stochastic_gradient_descent(X)\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid optimization algorithm.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [26], line 226\u001b[0m, in \u001b[0;36mBinaryRBM._stochastic_gradient_descent\u001b[1;34m(self, _data)\u001b[0m\n\u001b[0;32m    224\u001b[0m accum_delta_c[:] \u001b[39m=\u001b[39m \u001b[39m.0\u001b[39m\n\u001b[0;32m    225\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m--> 226\u001b[0m     delta_W, delta_b, delta_c \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_contrastive_divergence(sample)\n\u001b[0;32m    227\u001b[0m     accum_delta_W \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m delta_W\n\u001b[0;32m    228\u001b[0m     accum_delta_b \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m delta_b\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense, LSTM, MaxPool1D, Flatten, Dropout # importing dense layer\n",
    "from keras.models import Sequential #importing Sequential layer\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "# representation of model layers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "feature=[\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\n",
    "          \"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\n",
    "          \"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\n",
    "          \"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
    "          \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"]\n",
    "\n",
    "train='./nslkdd/KDDTrain+.txt'\n",
    "train_data=pd.read_csv(train,names=feature)\n",
    "data= train_data\n",
    "# remove attribute 'difficulty_level'\n",
    "data.drop(['difficulty'],axis=1,inplace=True)\n",
    "def change_label(df):\n",
    "        df.label.replace(['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm'],'Dos',inplace=True)\n",
    "        df.label.replace(['ftp_write','guess_passwd','httptunnel','imap','multihop','named','phf','sendmail','snmpgetattack','snmpguess','spy','warezclient','warezmaster','xlock','xsnoop'],'R2L',inplace=True)      \n",
    "        df.label.replace(['ipsweep','mscan','nmap','portsweep','saint','satan'],'Probe',inplace=True)\n",
    "        df.label.replace(['buffer_overflow','loadmodule','perl','ps','rootkit','sqlattack','xterm'],'U2R',inplace=True)\n",
    "\n",
    "change_label(data)\n",
    "label = pd.DataFrame(data.label)\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "def standardization(df,col):\n",
    "    for i in col:\n",
    "        arr = df[i]\n",
    "        arr = np.array(arr)\n",
    "        df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
    "    return df\n",
    "\n",
    "numeric_col = data.select_dtypes(include='number').columns\n",
    "data = standardization(data,numeric_col)\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "enc_label = label.apply(le2.fit_transform)\n",
    "data['intrusion'] = enc_label\n",
    "print(data.shape)\n",
    "data.drop(labels= ['label'], axis=1, inplace=True)\n",
    "print(data.shape)\n",
    "data = pd.get_dummies(data,columns=['protocol_type','service','flag'],prefix=\"\",prefix_sep=\"\")  \n",
    "print(data.shape)\n",
    "y_data= data['intrusion']\n",
    "X_data= data.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_train has shape:',X_data.shape,'\\ny_train has shape:',y_data.shape)\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "y_data = LabelBinarizer().fit_transform(y_data)\n",
    "\n",
    "X_data=np.array(X_data)\n",
    "y_data=np.array(y_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data,y_data, test_size=0.25, random_state=0)\n",
    "\n",
    "# Build the DBN model\n",
    "model = SupervisedDBNClassification(hidden_layers_structure=[256, 128, 64],\n",
    "            learning_rate_rbm=0.05,\n",
    "            learning_rate=0.1,\n",
    "            n_epochs_rbm=10,\n",
    "            n_iter_backprop=100,\n",
    "            batch_size=32,\n",
    "            activation_function='relu',\n",
    "            dropout_p=0.2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "125580dea55cd538c9a78e6a4677c236a4b84c97a3552c27a8deeb92c7556164"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
